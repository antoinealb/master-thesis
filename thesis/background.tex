\chapter{Background}

\section{Distributed Consensus}

In recent years, datasets have become larger and larger.
Internet-scale companies started using horizontal scaling, in which a large number of inexpensive machines is used.
This approach allowed them to keep up with the increase of sets, but also introduced challenges regarding data consistency.
For example, in a distributed database, some nodes could see transaction A before transaction B, while other nodes could see them in a different order.
If only one of the transaction could proceed, this would create an issue.

Horizontal scaling also created issues of \emph{availability}.
The increased number of servers meant an increase in the probabiltiy of a node failure.

Distributed consensus is the problem of agreeing on a shared state in face of unreliable communications.
It is understandable then that once solution to the distributed consensus problems were available, they quickly became parts of almose every modern large scale system.

why does it matter?

\begin{enumerate}
    \item datasets become bigger 
    \item horizontal scaling using commodity hardware is more convenient and cheaper than large mainframes.
    \item however the parallelism it creates means new issues arise.
    \item especially: how do you ensure consistency of data?
        Maybe one node executes A then B while the other one does B then A.
    \item Also creates issues of fault tolerance; many systems, each with relatively low individual resilience.
    \item The problem of agreeing ona value is called ``distributed consensus problem''
\end{enumerate}

Maybe something about state machine replication?


\subsection{Raft Consensus Protocol}

Raft\cite{raft} is a protocol that proposes a solution to the distributed consensus problem.
Its main goal is to be easy to understand while still being efficient.
It achieves its objective by cleanly separating different concerns of the protocol.
It also provides a strong leader, i.e. all entries flow from the leader to the followers, making it simpler to reason about.
Unlike the original Paxos, Raft provides a complete solution to build a replicated log.

At its core, Raft is made of three different parts: leader election, log entries replication and commit propagation.
Each node can be in one of three states: \emph{follower}, \emph{candidate} or \emph{leader}.
During normal operation (when a leader emerged), a cluster contains exactly one leader and zero candidates.
We will start by assuming that everything is normal to explain how the protocol works, then explain what changes during leader election.

SOMETHING ABOUT TERMS
* exactly one leader per term

Each entry in the Raft replicated log is tagged with an \emph{index}, which is a monotonic counter, and a term number.

\subsection{Log Replication}

When the leader receives a request from a client, it appends it to its local log, along with its index and term.
It will then periodically send \emph{AppendEntries} requests to the followers.
Each of those contains all the entries that were not yet acknowledged by the destination follower.
It also contains the term and index of the entries immediately before the first one in the request.
This allows a follower to detect any gap between its local log and the incoming entries.

The destination follower will then append the new entries contained in the \emph{AppendEntries} request to its own log.
During this step, the entries' terms and indices are used to detect inconsistencies and duplicated entries.
It then sends a reply to the leader to acknowledge that the incoming entries were correctly replicated.
It can also notify the leader of a failure to replicate, for example due to a gap in the log.

Note that the checks performed when processing \emph{AppendEntries} requests guarantee that, if two log entries on two different machines have the same index and term, then:

\begin{enumerate}
    \item They store the same operation.
    \item All previous log entries are identical in the two logs.
\end{enumerate}

Those two properties are very useful when considering committing.

\subsection{Entries commit}

We say that an entry is \emph{committed} when we know for sure that this entry will never be lost by the cluster\footnote{Provided that a majority of server stay healthy.}.
We also know that a committed entry will eventually be replicated to the whole cluster.
This means that to be committed, an entry must be replicated on a majority of servers.
When an entry is marked as committed, its content can be consumed by the application.

Raft does not keep track of the commit status for each individual entry.
Instead, it tracks the last committed entry and defines that entries before it are committed as well.

The leader keeps track of the latest acknowledged entry for each follower.
Once an entry has been replicated on a majority of machines, the leader moves its commit index to it, and tells the followers about the new commit index.


\subsection{Leader Election}

In Raft, leader election is based on timers.
First, a periodic timer is used to send heartbeats to followers.
Those heartbeats are \emph{AppendEntries} requests, which can be empty if there is no new request to replicate.
Every time one of those heartbeats is received, the follower resets the second timer, called the \emph{election timeout} timer.

If no messages is received from the leader, then the election timer will fire.
The follower can then then start a new leader election.
It will first increment the term, as there can be only one leader per term.
It will then transition to the candidate state and send \emph{Vote} requests to other cluster participants.

Once a node receives a \emph{Vote} request, it decides wether or not to grant its vote to the requesting candidate.
To do so, it will first check that the candidate's term is more recent than its own.
It will also check that the candidate's log is at least as complete as its own.
\footnote{This ensures that the leader's log contains all committed entries.
    If this was not the case, then the leader would start replacing committed entries, leading to loss of consistency.
}
Finally, it sends a reply to the candidate containing wether or not it grants its vote to the candidate.

If a candidate reaches majority, then it transitions to the leader role and starts sending heartbeats.
If no majority emerges, then the election timeout will fire again, restarting the process at a new term.
To avoid conflicting elections where no majority can occur, the timeout duration is randomized, so that election will eventually succeed.


\section{Transport protocols}

R2P2

\subsection{Request Response Pair Protocol (R2P2)}

Most current \gls{rpc} systems, such as Google's gRPC\cite{grpc} or Facebook's Thrift\cite{thrift} typically use TCP as their transport layer.
This choice makes sense when running on the internet: packets may be lost or re-ordered, which TCP handles gracefully, and TCP's added latency is probably acceptable.
However, when running inside a datacenter, packet loss or re-ordering is not as likely.
In addition, TCP's latency is an issue for distributed systems, as a single user's request might generate many backend \gls{rpc} calls.

Based on those observations, the \gls{dcsl} developped a new transport protocol specially designed for \glspl{rpc}.
Unlike TCP, this new protocol is connectionless; each communication is made of a single request followed by a single response, hence the name of \gls{r2p2}.
This reduces latency by removing the handshake round trip time of TCP.
\gls{r2p2} uses UDP datagrams to send the requests or the reponses.
An advantage of this approach is that the server that answers does not need to be the one the request was originally for.
This has been succesfully used in the past to implement new load balancing techniques\cite{r2p2}.

An interesting difference of \gls{r2p2} is that each request includes a field to specify how it should be routed.
For example, it can be marked as ``Fixed'', meaning that the request must be served by the receiver, or ``load-balance'', in which case the router is free to redirect it to another server.
We realized that it could be interesting to embed the replication mechanism in the transport layer.
That way, a request could be marked as ``Replicated'', and then the server receiving it would automatically replicate it to other machines.
Once the request is marked as ``committed'' by the consensus protocol, the request will be forwarded to the application layer.
This means every node in the cluster would eventually see this request, and that all nodes would see the same order.

Bringing the consensus protocol to the transport layer greatly simplifies the life of the application developper.
Any networked application can be turned into a replicated version of it simply by changing a flag on the requests.
Clients could even choose wether or not to ask for replication based on application-specific logic.
For example, if stales read from a key-value store are acceptable, read request could be marked as ``load-balanced'', while write requests would be marked as ``replicated'' to ensure consistency.

\subsection{R2P2 API}

The \gls{r2p2} library \gls{api} looks nothing like a BSD socket \gls{api}.
Instead it is \emph{event oriented}: the user of the library provides a set of callbacks that are called when a request is received (in the case of a server) or when a response arrives (in the case of a client).
Those callbacks also receive a per-connection argument which can be used to track state.
The core of the \gls{r2p2} \gls{api} can be seen in Listing~\ref{listing:r2p2-client-api} and \ref{listing:r2p2-server-api}.

Such event-oriented \gls{api} maps well to the semantics of either high performance I/O syscalls (e.g. Linux's \texttt{epoll}) or the one of kernel bypass frameworks such as DPDK.

\begin{lstfloat}
\lstinputlisting[label=listing:r2p2-client-api,caption={\gls{r2p2} \gls{api} summary}]{code_snippets/r2p2_client_api.c}
\end{lstfloat}

\begin{lstfloat}
\lstinputlisting[label=listing:r2p2-server-api,caption={\gls{r2p2} server \gls{api}}]{code_snippets/r2p2_server_api.c}
\end{lstfloat}

With our proposal of bringing the consensus protocol to the transport layer, switching a normal application to a distributed, consistent one is simply a matter of changing the \texttt{routing\_policy} field from \texttt{FIXED\_ROUTE} to \texttt{REPLICATED\_ROUTE}.
We hope that this will create a re-usable framework and that more developper will be able to write fault tolerant systems as a result.

\section{Kernel Bypass}

It is well known in the system engineering community that context switching between userland and kernel space can be expensive.
For example, Google observed a 3x gain in throughput when bypassing the Linux kernel\cite{maglev}.

To reduce the number of switches between kernel space and userland, we have two choices: moving more parts of the system in the kernel, or moving more parts of the system in userland.  The first option is the one that has been traditionnaly been used for networking; the TCP/IP stack or the filesystem are part of typical UNIX kernels.
However, moving code in kernel space is not an easy task: kernel code usually has very little to no memory protection.
This causes security threats and makes debugging harder.

The second option, moving more parts of the system in userland is known as \emph{kernel bypass}.
In this technique, the application embeds everything it needs, from \gls{nic} drivers to TCP.
The main downside of this technique is that it prevents sharing of ressources between applications running on the server.
For example, each networked application would need its own \gls{nic}.

A key contribution of our work is the use of kernel bypass techniques to reduce latency.
In particular, we are opposing our design to Kernel Paxos\cite{kernelpaxos}, which moved the Paxos consensus protocol in the Linux kernel with great results.
We believe that using kernel bypass techniques can lead to similar, if not better performance without compromising process separation.


