\chapter{Future work}

\section{Disk backed persistency}

The current implementation only stores requests in memory.
This means that requests are not truly persistent: if all nodes were to come offline, committed data would be lost.
To avoid this, the replicated log would need to be persisted to non volatile storage.

However our application cannot use the standard Linux filesystem API.due to the latency of syscalls.
Recently new technologies have emerged to use persistent storage (particularly \glspl{ssd}) without relying on the kernel.
One of those approach is Intel's SPDK\cite{spdk}, which uses kernel bypass techniques and presents an API similar to DPDK.
Another option would be to use the newly introduced Linux's \texttt{IOCTX\_FLAG\_SQTHREAD} flag for polled IO.
This works by submitting filesystem operations in a ring buffer, which will get collected and executed by the kernel later.

\section{Max-latency batching}

Random idea: Currently requests are either immediately replicated to the log or we wait for a configurable timeout to expire, then send a batch of request.
This could lead to bad behaviour in heavily loaded servers (many small appendrequests) or create timeouts that are too important for the application.
Maybe a smart batching policy is possible and should be designed in the protocol.

\section{Consistency verification of the protocol}

Jepsen\cite{jepsen} is an automated tool for checking of consistency properties in distributed systems.
It works by randomly applying partitions, delays and packet reordering to distributed applications.
So far it sucessfuly analyzed big open source project such as MongoDB, etcd, or Redis.
We think we would gain a lot of confidence in the soundness of our implementation if a Jepsen testbench was available for it.
It could be written using \gls{r2p2} as its only interface, so that the same test suite could be shared between many different \gls{r2p2} implementations.

\chapter{Conclusion}

