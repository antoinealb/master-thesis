\chapter{Design}
\label{chap:design}

\section{Choice of a consensus protocol}

One of the earliest solution to the consensus problem is Lamport's Paxos algorithm\cite{paxos}.
It is currently widely used, for example Google relies on it to ensure correctness of its distributed systems\cite{chubby, paxoslive}.
However, Paxos has two issues: first, it only solves part of the problem; some other ``bricks'' must be added to it to form a complete system.
In addition to this, Paxos is known to be hard to understand, and even harder to implement correctly.
Most of its users rely on pre-existing implementations such as libpaxos\footnote{\url{https://bitbucket.org/sciascid/libpaxos/}}.
This means modifying the consensus protocol for our application and chosen transport could be hard to do

Fortunately for us, a simpler alternative to Paxos called Raft was recently introduced\cite{raft}.
Raft was designed from the ground up to be simpler by cleanly separating the different parts of the problem.
It also provides a complete solution, unlike Paxos which often requires additional, unproven extensions\cite{paxoslive}.

Another possible choice would have been ZooKeeper's ZAB\cite{zookeeper}, but it was not as well documented as Raft.
In addition, it appeared to provide primitives that were less general purposes than a replicated log.

Despite the availability of excellent production grade Raft implementations, we decided to implement our own.
Most existing Raft implementations are using high level languages such as Go, which are not suited for low latency programming.
Other implementations in C or C++ make a lot of assumptions on the underlying platform.
For all those reasons, it was deemed easier to do it ourself, using experience gathered by doing a prototype in Python.
The resulting implementation is pretty small, at about 1000 lines of C++ code.

\section{Modifications to R2P2}


\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{r2p2_header}
    \caption{\gls{r2p2} header format\cite{r2p2}}
    \label{fig:r2p2-header}
\end{figure}


\Gls{r2p2} requests have a \emph{Policy} field to indicate to load balancers how this request should handled (Figure~\ref{fig:r2p2-header}).
The pre-existing policies were \texttt{LB\_ROUTE} (can be redirected to any backend) or \texttt{FIXED\_ROUTE} (the request should really be handled by the destination backend).
We added the \texttt{REPLICATED\_ROUTE} policy: all the backends will eventually receive this request.
In addition, all backends are guaranteed to receive the requests using this policy in the same order.

Since this policy field is per request, it means clients can selectively enable request replication.
For example, a user might specify that no write should be lost but that stale reads are acceptable (to lower latency).
Then write requests would be sent with the \texttt{REPLICATED\_ROUTE} policy, while reads are sent with \texttt{LB\_ROUTE}.

Another addition to the R2P2 protocol was to add a new \emph{MessageType} for Raft-related messages.
The different types of Raft messages are differentiated in the \gls{r2p2} payload using a type field.
The list of messages and their description can be found below.
In addition, each message contains the term number of the sender, as it is required for all Raft operations.

\begin{description}
    \item[VoteRequest] Fields: \texttt{last\_log\_index}, \texttt{last\_log\_term}.
        Sent from one candidate to a follower to request a vote for itself.
        The candidate includes the last log entry's index and term so that followers can check if the candidate's log is up to date.
    \item[VoteReply] Fields: \texttt{vote\_reply}.
        Reply to a vote request, sent from a follower to a candidate.
        Contains a boolean indicating wether or not the follower granted its vote.
    \item[AppendEntriesRequest] Fields: \texttt{count}, \texttt{leader\_commit}, \texttt{previous\_entry\_term}, \texttt{previous\_entry\_index}, \texttt{entries}.
        This message is sent from the leader to the followers when there is new entries to add to the log, or periodically as a heartbeat if there is no activity.
        It contains data about the entry immediately before the provided one so that followers can check for gap in their log.
        It also serves to forward the commit index information to followers.
    \item[AppendEntryReply] Fields: \texttt{success}, \texttt{last\_index}.
        This message is sent from followers to leader and contains wether or not the new entries were successfully applied to the log, as well as the last index in the follower's log.
        This information can then be used by the leader to decide which entries to send next, as well as update the commit index.
\end{description}



\section{Request lifecycle}

\begin{figure}[hp]
    \centering
    \includegraphics[width=0.8\textwidth]{client_server_interaction}
    \caption{Typical client server interaction for a replicated service call.
        Request arrives to the leader (1).
        Request is then replicated to all followers (2).
        Followers acknowledge the replication (3).
        Leader marks the request committed (4).
        Commited requests are forwarded to the application and reply is sent to the client (5).
        Note that all arrows are complete \gls{r2p2} messages: if they span multiple UDP datagrams, we wait for the complete message to arrive before replicating it.
    \label{fig:client-server-interaction}
    }
\end{figure}

Figure~\ref{fig:client-server-interaction} shows how a replicated request is handled.
In this example, the first node is the leader, and therefore the request must be directed to it (1).
The request is then packed in a log entry, and sent to the followers through Raft's \emph{AppendEntryRequest} message (2).
Nodes then send back an \emph{AppendEntryReply} message, acknowledging the message to the leader (3).
Once a request has been acknowledged by a majority of the nodes, the leader marks it as committed.
From this point on, the request will not be lost by the cluster\footnote{Provided that a majority of nodes stay healthy}.
Therefore, the request is forwarded to the application and the reply sent to the client (5).
On next message, or on timeout, the new commit index will also be sent to followers (4).
At this point they also forward it to their local copy of the application, which also sends back answers(5).
The replies sent from non-leader are silently dropped, as they would arrive after the leader's reply.
